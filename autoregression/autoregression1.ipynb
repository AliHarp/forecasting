{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive model using OLS\n",
    "\n",
    "In this notebook we will use Ordinary Least Square (OLS) to fit a linear model to time series data. OLS is a supervised learning approach and is not designed specifically with time series in mind.  However, it is easy to adapt and use with autoregressive data.  There are two caveats \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "1. Unlike purpose built time series models such as ARIMA we need to preprocess the data beforehand.\n",
    "\n",
    "2. Forecasting **h-steps** ahead is more involved than other methods.\n",
    "</div>\n",
    "\n",
    " \n",
    "However, OLS itself is very simple to use and the procedures we develop to solve 1 and 2 are exactly what we need to work with Feedforward Neural Networks and Recurrent Neural Networks which we will cover later.\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Mechanics of fitting time series data to an OLS autoregressive model\n",
    "* Generate h-step forecasts using an iterative approach\n",
    "* Generate h-step forecast using a direct modelling approach\n",
    "* Generate prediction intervals for an OLS autoregressive model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided for this work.  The implementation of Ordinary Least Squares Regression that we are going to use is in `statsmodels`.  You should be using at least version `0.11.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sm.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodels` package has a class for OLS that we are going to use.\n",
    "\n",
    "```python \n",
    "statsmodels.regression.linear_model.OLS \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AR OLS: The forecasting process\n",
    "\n",
    "1. Select $l$ the number of autoregressive lags and forecast horizon $h$\n",
    "2. Preprocess the data into tabular (supervised learning) form [[$lag_1, lag_2, ... lag_l$], [$y_t$]]\n",
    "3. Fit the OLS model to the tabular data\n",
    "4. Iteratively forecast 1-step ahead gradually replacing ground truth observations with predictions.\n",
    "\n",
    "\n",
    "### 2.1 Synthetic data without noise\n",
    "\n",
    "Given the extra complexities of forecasting using OLS we will use simple synthetic data before exploring real healthcare data. The synthetic data we wil use is a cosine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocess the time series into tabular autoregressive form\n",
    "\n",
    "An autoregressive model consists of $l$ lags of the time series. \n",
    "\n",
    "An easy way to think about the form of the data for a autoregressive OLS model is as a table of variables.  The first $l$ columns are the lags (the independent predictor variables) and the final column is $y$ at time $t$ ($y_t$) that is, the target/dependent variable.  \n",
    "\n",
    "We there need to manipulate the time series so that is now in that format.  More precisely for each row we need: \n",
    "\n",
    "**A vector presenting the lags at time t**\n",
    "* $X_t = $ [$lag_{t-l}, ... lag_{t-2}, lag_{t-1}$]\n",
    "\n",
    "**A scalar value representing y at time t:**\n",
    "* $y_t$\n",
    "\n",
    "For training we need a vector of rows ($X_t$) and vector of target $y_t$. e.g.\n",
    "\n",
    "```python\n",
    "X_train = [X_1, X_2, X_3, ...,  X_t]\n",
    "\n",
    "y_train = [y_1, y_2, y_3, ..., y_t]\n",
    "```\n",
    "---\n",
    "\n",
    "The function `sliding_window` illustrates how to preprocess time series data into tabular form  in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do a train test split first\n",
    "train = ts_data[:175]\n",
    "test = ts_data[175:]\n",
    "\n",
    "#preprocess time series training and test sets\n",
    "X_train, y_train = sliding_window(train, window_size=2)\n",
    "X_test, y_test = sliding_window(test, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing included 2 lags.  So let's have a look at how the first elements in `X_train` and `y_train` compare to the first three elements of `ts_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ts_data[:3]: {ts_data[:3]}')\n",
    "print(f'X_train[0]: {X_train[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to prove this is work let's manually slide out window of lag size 2 along `ts_data` to see how it compares with the next element in `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ts_data[1:4]: {ts_data[1:7]}')\n",
    "print(f'X_train[1]: {X_train[1]}')\n",
    "print(f'y_train[1]: {y_train[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Just to be absolutely clear what we are doing let's convert the numpy arrays to a **pandas.DataFrame**.  In this format you should be able to see why the an OLS model\n",
    "can be used for autoregression in a time series.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_form = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "columns = [f'lag_{i}' for i in range(len(X_train[0]), 0, -1)]\n",
    "columns.append('y_t')\n",
    "tabular_form.columns = columns\n",
    "tabular_form.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Fit the preprocessed data to the OLS model\n",
    "\n",
    "After preprocessing the data, fitting the data is relatively straightforward.  We create an instance of `OLS` passing in the training data and call the `.fit()` method.  The method fit returns a `RegressionResults` object that we use for prediction.  We can called the `.summary()` method to see the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "model = OLS(endog=y_train, exog=X_train).fit()\n",
    "\n",
    "#see the regression results (not the adj. R squared!)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecasting 1 step ahead\n",
    "\n",
    "To forecast 1-step ahead we use the `RegressionResults` method `.predict(exog)`.  For example if we wanted to forecast the first y observation in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sm.add_constant(X_test)\n",
    "pred = model.predict(exog=X_test[0])[0]\n",
    "print(f'1-step forecast: {pred}')\n",
    "print(f'ground trust value: {y_test[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to call predict is to use `.get_prediction(exog)` to get a `PredictionResults` object.  This has the nice method `.summary_frame(alpha=0.05)` that can be used to get a dataframe with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.get_prediction(exog=X_test[0])\n",
    "results.summary_frame(alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecast h periods ahead using the iterative method.\n",
    "\n",
    "**We have trained our `OLS` model to predict 1-step** ahead by regressing the previous two observations in the time series.  When forecasting 2 or more steps ahead we still only have two ground truth observations ($lag_1$ and $lag_2$). This means that when forecasting h-steps ahead we need to do this in a loop where we iteratively replace our ground truth observations with our predictions.\n",
    "\n",
    "There's an easy way to do this in python using the `np.roll(a, shift)` function.  That shifts everything in the array down by `shift`.  The function is **circular** so the value in element 0 is moved to be the final value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_X = np.array([1, 2, 3, 4])\n",
    "current_X = np.roll(current_X, shift=-1)\n",
    "current_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_X = np.array([1, 2, 3, 4])\n",
    "current_X = np.roll(current_X, shift=-1)\n",
    "current_X[-1] = y_pred\n",
    "current_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X)[0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = np.roll(current_X, shift=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 5\n",
    "y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "print(f'Iterative forecast: {y_preds}')\n",
    "print(f'Ground truth y: {y_test[:H].T[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this a bit more interesting we will add some normally distributed noise to the synthetic time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the random seed so that we all get the same results\n",
    "np.random.seed(12)\n",
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)\n",
    "noise = np.random.normal(loc=0.0, scale=0.3, size=200)\n",
    "ts_data = ts_data + noise\n",
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_via_aic(train, test, window_sizes):  \n",
    "    best = {'aic':np.Inf,\n",
    "            'model': None,\n",
    "            'ws': None}\n",
    "    \n",
    "    for ws in window_sizes:\n",
    "    \n",
    "        #preprocess time series training and test sets\n",
    "        X_train, y_train = sliding_window(train, window_size=ws)\n",
    "        X_test, y_test = sliding_window(test, window_size=ws)\n",
    "\n",
    "        X_train = sm.add_constant(X_train)\n",
    "        model = OLS(endog=y_train, exog=X_train).fit()\n",
    "        \n",
    "        if model.aic < best['aic']:\n",
    "            best['aic'] = model.aic\n",
    "            best['model'] = model\n",
    "            best['ws'] = ws\n",
    "            \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = [i for i in range(2, 20, 2)]\n",
    "print(f'Window sizes to test {ws}')\n",
    "\n",
    "#train test split 150 and 50\n",
    "train, test = ts_data[:150], ts_data[150:]\n",
    "\n",
    "best = select_model_via_aic(train, test, ws)\n",
    "print(f\"lags included: {best['ws']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data[best['ws']+1:], label='ground truth')\n",
    "plt.plot(best['model'].fittedvalues, label='AR OLS fitted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup test data\n",
    "ws = best['ws']\n",
    "\n",
    "#preprocess time series training and test sets\n",
    "X_train, y_train = sliding_window(train, window_size=ws)\n",
    "X_test, y_test = sliding_window(test, window_size=ws)\n",
    "\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "model = OLS(endog=y_train, exog=X_train).fit()\n",
    "\n",
    "#make iterative predictions\n",
    "H = len(y_test)\n",
    "y_preds_iter = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "\n",
    "#plot\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The direct h-step forecasting method.\n",
    "\n",
    "In the direct method to forecast h-steps ahead we have **$h$ forecasting models**.  Each model provides a single point forecast from a step ahead.  In the example here, y_test is of length 31 periods.  The direct method requires 31 OLS models to make its prediction!\n",
    "\n",
    "\n",
    "Recall the `sliding_window` function.  We ignored an optional parameter `horizon` in the iterative example.  By default `horizon=1` i.e. the function returns target values that are only a single period ahead.  We can vary the step size by increasing the value of horizon.  \n",
    "\n",
    "Let's start by assigning `horizon` the value 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the X and y training data\n",
    "X_train, y_train = sliding_window(train, window_size=2, horizon=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the first 5 values in the original time series.  \n",
    "#Note that y_train[0] is 2 steps ahead of X_train[0]\n",
    "print(f'ts_data[:5]: {ts_data[:5]}')\n",
    "print(f'X_train[0]: {X_train[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assign `horizon` the value 3 we get the same behaviour, but `y_train[0]` is now 3 steps ahead of `X_train[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = sliding_window(train, window_size=2, horizon=3)\n",
    "print(f'ts_data[:5]: {ts_data[:5]}')\n",
    "print(f'X_train[0]: {X_train[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training multiple models**\n",
    "\n",
    "For OLS it is simple and quick to train the 31 models.  \n",
    "\n",
    "1. Create a for loop and set it to iterate 31 times. \n",
    "2. In each loop call `sliding_window` setting `horizon` to the iteration number + 1\n",
    "3. Train the model and save in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "horizon = 31\n",
    "\n",
    "for h in range(horizon):\n",
    "    X_train, y_train = sliding_window(train, window_size=18, horizon=h+1)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    model_h = OLS(endog=y_train, exog=X_train).fit() \n",
    "    models.append(model_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `direct_forecast` function.  This is just a for loop to call the `.predict(exog)` method of each model.  Remember that the input to each model is **same** i.e. exog which in our case will be `X_test[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_forecast(models, exog):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the direct prediction method.\n",
    "    \n",
    "    Each model contained in @models has been trained\n",
    "    to predict a unique number of steps ahead. \n",
    "    Each model forecasts and the results are \n",
    "    combined in an ordered array and returned.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    models: list\n",
    "        direct models each has has a .predict(exog) \n",
    "        interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "        \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    preds = []\n",
    "    for model_h in models:\n",
    "        pred_h = model_h.predict(exog=exog)\n",
    "        preds.append(pred_h)\n",
    "    \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clarity let's recreate the X and y test data\n",
    "X_test, y_test = sliding_window(test, window_size=18)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "#make the direct forecast\n",
    "y_preds_direct = direct_forecast(models, X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the direct forecast against the test data\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the iterative method the direct method looks a close match to the ground truth test set!  Let's plot all three datasets on the same chart and then take a look at the **RMSE** of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot iterative and direct\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_preds_iter.reshape(-1, 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_preds_direct)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example (and holdout set) the direct method out performed the iterative method. You should not assume this is always the case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
