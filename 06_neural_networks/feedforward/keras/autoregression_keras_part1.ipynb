{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive model using a feedforward neural network\n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a linear model to time series data. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "1. The data preprocessing requirements for a NN are similar to those of an OLS.\n",
    "\n",
    "2. Forecasting **h-steps** ahead can use either an iterative or direct method.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Use a NN to mimic a linear model\n",
    "* Generate h-step forecasts using an iterative approach\n",
    "* Generate h-step forecast using a direct modelling approach\n",
    "* Construct a deep feedforward neural network for forecasting\n",
    "* Use a ensemble of neural networks to forecast\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided for this work. We are going to implement neural networks using `tensorflow` and '`keras`. You should be using at least `tensorflow` version `2.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reminder: The forecasting process for AR\n",
    "\n",
    "1. Select $l$ the number of autoregressive lags and forecast horizon $h$\n",
    "2. Preprocess the data into tabular form [[$lag_1, lag_2, ... lag_l$], [$y_t$]]\n",
    "3. Train the NN model using the tabular data\n",
    "4. Iteratively forecast 1-step ahead gradually replacing ground truth observations with predictions.\n",
    "\n",
    "\n",
    "### 2.1 Synthetic data without noise\n",
    "\n",
    "Given the extra complexities of forecasting using OLS we will use simple synthetic data before exploring real healthcare data. The synthetic data we wil use is a cosine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocess the time series into tabular autoregressive form\n",
    "\n",
    "An autoregressive model consists of $l$ lags of the time series. \n",
    "\n",
    "An easy way to think about the form of the data for a autoregressive OLS model is as a table of variables.  The first $l$ columns are the lags (the independent predictor variables) and the final column is $y$ at time $t$ ($y_t$) that is, the target/dependent variable.  \n",
    "\n",
    "We there need to manipulate the time series so that is now in that format.  More precisely for each row we need: \n",
    "\n",
    "**A vector presenting the lags at time t**\n",
    "* $X_t = $ [$lag_{t-l}, ... lag_{t-2}, lag_{t-1}$]\n",
    "\n",
    "**A scalar value representing y at time t:**\n",
    "* $y_t$\n",
    "\n",
    "For training we need a vector of rows ($X_t$) and vector of target $y_t$. e.g.\n",
    "\n",
    "```python\n",
    "X_train = [X_1, X_2, X_3, ...,  X_t]\n",
    "\n",
    "y_train = [y_1, y_2, y_3, ..., y_t]\n",
    "```\n",
    "---\n",
    "\n",
    "The function `sliding_window` illustrates how to preprocess time series data into tabular form  in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess time series training and test sets\n",
    "X_train, y_train = sliding_window(ts_data, window_size=2)\n",
    "\n",
    "training_length = int(len(y_train) * (2/3))\n",
    "\n",
    "X_train, X_test = X_train[:training_length], X_train[training_length:]\n",
    "y_train, y_test = y_train[:training_length], y_train[training_length:]\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "If you need a reminder of what the preprocessing does to the data please look back to the OLS notebook.  Here is the result of the preprocessing as a **pandas.DataFrame**.  This looks just like cross-sectional data that you would use to train a conventional NN.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_form = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "columns = [f'lag_{i}' for i in range(len(X_train[0]), 0, -1)]\n",
    "columns.append('y_t')\n",
    "tabular_form.columns = columns\n",
    "tabular_form.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train the NN using the preprocessed series\n",
    "\n",
    "After preprocessing the data, fitting the data is relatively straightforward.  We create an instance of `OLS` passing in the training data and call the `.fit()` method.  The method fit returns a `RegressionResults` object that we use for prediction.  We can called the `.summary()` method to see the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_model(ws, lr=0.01, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = ['mae', 'mse']\n",
    "    \n",
    "    model = Sequential([Dense(1, input_shape=(ws,))])\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(lr=lr),\n",
    "                  metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 5\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "#pre-process the data into sliding windows\n",
    "X_train, y_train = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "#train test split\n",
    "training_length = int(len(y_train) * (2/3))\n",
    "X_train, X_test = X_train[:training_length], X_train[training_length:]\n",
    "y_train, y_test = y_train[:training_length], y_train[training_length:]\n",
    "\n",
    "#compile the tf model\n",
    "model = get_linear_model(WINDOW_SIZE)\n",
    "\n",
    "history = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecasting 1 step ahead\n",
    "\n",
    "To forecast 1-step ahead we use the `RegressionResults` method `.predict(exog)`.  For example if we wanted to forecast the first y observation in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x=X_test[0].reshape(1, -1))[0,0]\n",
    "print(f'1-step forecast: {pred}')\n",
    "print(f'ground trust value: {y_test[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Forecast h periods ahead using the iterative method.\n",
    "\n",
    "**We have trained our `NN` model to predict 1-step**. When forecasting 2 or more steps ahead we still only have five ground truth observations ($lag_1$ ... $lag_5$). This means that when forecasting h-steps ahead we need to do this in a loop where we iteratively replace our ground truth observations with our predictions.\n",
    "\n",
    "There's an easy way to do this in python using the `np.roll(a, shift)` function.  That shifts everything in the array down by `shift`.  The function is **circular** so the value in element 0 is moved to be the final value in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X.reshape(1, -1), \n",
    "                               use_multiprocessing=True)[0,0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = np.roll(current_X, shift=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 5\n",
    "y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "print(f'Iterative forecast: {y_preds}')\n",
    "print(f'Ground truth y: {y_test[:H].T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding some noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this a bit more interesting we will add some normally distributed noise to the synthetic time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the random seed so that we all get the same results\n",
    "np.random.seed(12)\n",
    "t = np.arange(200)\n",
    "ts_data = np.cos(0.2 * t)\n",
    "noise = np.random.normal(loc=0.0, scale=0.2, size=200)\n",
    "ts_data = ts_data + noise\n",
    "plt.plot(ts_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 12\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#preprocess time series training and test sets\n",
    "X_train, y_train = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "training_length = 130\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:training_length], X_train[training_length:]\n",
    "y_train, y_test = y_train[:training_length], y_train[training_length:]\n",
    "\n",
    "#compile the tf model\n",
    "model = get_linear_model(WINDOW_SIZE, metrics=['mse'], lr=0.001)\n",
    "\n",
    "#fit model silently\n",
    "history = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_data[WINDOW_SIZE+1:], label='ground truth')\n",
    "plt.plot(model.predict(X_train), label='NN fitted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make iterative predictions\n",
    "H = len(y_test)\n",
    "y_preds_iter = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "\n",
    "#plot\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.5 The direct h-step forecasting method.\n",
    "\n",
    "In the direct method to forecast h-steps ahead we have **$h$ forecasting models**.  Each model provides a single point forecast from a step ahead.  In the example here, y_test is of length 57 periods.  The direct method requires 57 NNs to make its prediction!\n",
    "\n",
    "\n",
    "Recall the `sliding_window` function.  We ignored an optional parameter `horizon` in the iterative example.  By default `horizon=1` i.e. the function returns target values that are only a single period ahead.  We can vary the step size by increasing the value of horizon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training multiple models**\n",
    "\n",
    "1. Create a for loop and set it to iterate 57 times. \n",
    "2. In each loop call `sliding_window` setting `horizon` to the iteration number + 1\n",
    "3. Create a new instance of the the model\n",
    "4. Train the model and save in a list.\n",
    "5. Save the model to .h5 file. (recommended so you can reload without retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_direct_models(data, n_epochs, horizon, window_size, \n",
    "                        train_length, save=True):\n",
    "\n",
    "    models = []\n",
    "\n",
    "    print('Training model =>', end=' ')\n",
    "    for h in range(horizon):\n",
    "        print(f'{h+1}', end=', ')\n",
    "        #preprocess time series training and test sets\n",
    "        X_train, y_train = sliding_window(data, \n",
    "                                          window_size=window_size, \n",
    "                                          horizon=h+1)\n",
    "\n",
    "        #train-test split\n",
    "        X_train, X_test = X_train[:train_length], X_train[train_length:]\n",
    "        y_train, y_test = y_train[:train_length], y_train[train_length:]\n",
    "\n",
    "        #compile the tf model\n",
    "        model_h = get_linear_model(window_size, metrics=['mse'], lr=0.001)\n",
    "\n",
    "    \n",
    "        #fit model silently (verbose=0)\n",
    "        history_h = model_h.fit(x=X_train, \n",
    "                            y=y_train, \n",
    "                            epochs=n_epochs,\n",
    "                            verbose=0)\n",
    "        \n",
    "        if save:\n",
    "            model_h.save(f'./output/direct_model_h{h+1}.h5')\n",
    "\n",
    "        models.append(model_h)\n",
    "\n",
    "    print('done')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(horizon):\n",
    "    models = []\n",
    "    for h in range(HORIZON):\n",
    "        model_h = tf.keras.models.load_model(f'output/direct_model_h{h+1}.h5')\n",
    "        models.append(model_h)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #set tensorflow random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "HORIZON = len(y_test)\n",
    "WINDOW_SIZE = 12\n",
    "TRAIN_LENGTH = 130\n",
    "LOAD_FROM_FILE = True\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    direct_models = load_models(HORIZON)\n",
    "else:\n",
    "    direct_models = train_direct_models(ts_data, \n",
    "                                        n_epochs=N_EPOCHS,\n",
    "                                        horizon=HORIZON, \n",
    "                                        window_size=WINDOW_SIZE, \n",
    "                                        train_length=TRAIN_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the `direct_forecast` function.  This is just a for loop to call the `.predict()` method of each model.  Remember that the input to each model is **same** i.e. exog which in our case will be `X_test[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_forecast(models, exog):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the direct prediction method.\n",
    "    \n",
    "    Each model contained in @models has been trained\n",
    "    to predict a unique number of steps ahead. \n",
    "    Each model forecasts and the results are \n",
    "    combined in an ordered array and returned.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    models: list\n",
    "        direct models each has has a .predict(exog) \n",
    "        interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "        \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    preds = []\n",
    "    for model_h in models:\n",
    "        pred_h = model_h.predict(x=exog.reshape(1, -1), \n",
    "                                 use_multiprocessing=True)[0, 0]\n",
    "        preds.append(pred_h)\n",
    "    \n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the direct forecast\n",
    "y_preds_direct = direct_forecast(direct_models, X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clarity repeat the preprocessing\n",
    "X_train, y_train = sliding_window(ts_data, window_size=WINDOW_SIZE)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:TRAIN_LENGTH], X_train[TRAIN_LENGTH:]\n",
    "y_train, y_test = y_train[:TRAIN_LENGTH], y_train[TRAIN_LENGTH:]\n",
    "\n",
    "#plot the direct forecast against the test data\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_test, label='ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the iterative method the direct method looks a close match to the ground truth test set!  Let's plot all three datasets on the same chart and then take a look at the **RMSE** of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot iterative and direct\n",
    "plt.plot(y_preds_direct, label='direct forecast method')\n",
    "plt.plot(y_preds_iter, label='iterative forecast method')\n",
    "plt.plot(y_test, label='ground truth', linestyle='-.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_preds_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_test, y_preds_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example (and single holdout set) the iterative method out performed the direct method. You should not assume this is always the case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Forecasting a vector of y\n",
    "\n",
    "In the **iterative** and **direct** methods we always forecast a *scalar* value.  An modification is to adapt a feedforward neural network to predict a **vector of y values**.  Using this architecture we would train our model on sliding windows of $X$ and $y$.  Where y is a vector of length $h$ and $X$ is a vector of length $ws$ (window size)\n",
    "\n",
    "### 2.6.1 Exercise: preprocessing the time series into vectors of y\n",
    "Task: modify the function `sliding_window` (provided below) so that it returns a vectors of y.\n",
    "\n",
    "Hints:\n",
    "* Assume you are standing at time $t$. With a forecasting horizon of $h$, y would be $[y_{t+1}, y_{t+2}, ... , y_{t+h}]$.\n",
    "* Array slicing might prove useful:\n",
    "\n",
    "```python\n",
    "train = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(train[2:6])\n",
    "```\n",
    "```\n",
    ">> [3 4 5 6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=2):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=2)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        #we use list slicing to return a vector of training for y_train\n",
    "        y_train = train[i+window_size:i+window_size+horizon]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.array(tabular_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** you have modified `sliding_window` run the code below to preprocess the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 12\n",
    "HORIZON = 12\n",
    "TRAIN_LENGTH = 130\n",
    "#for clarity repeat the preprocessing\n",
    "X_train, y_train = sliding_window(ts_data, \n",
    "                                  window_size=WINDOW_SIZE,\n",
    "                                  horizon=HORIZON)\n",
    "\n",
    "#train-test split\n",
    "X_train, X_test = X_train[:TRAIN_LENGTH], X_train[TRAIN_LENGTH:]\n",
    "y_train, y_test = y_train[:TRAIN_LENGTH], y_train[TRAIN_LENGTH:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.2 Build setup a model that predicts vectors in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_vector_network_model(ws, n_inputs, outputs, n_neurons=32,\n",
    "                                    lr=0.01, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = ['mse']\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(ws,)))\n",
    "    model.add(Dense(n_neurons, activation='relu'))\n",
    "    model.add(Dense(outputs))\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=Adam(lr=lr),\n",
    "                  metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set tensorflow random seed\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "es = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#compile the tf model\n",
    "model_v = get_target_vector_network_model(WINDOW_SIZE, \n",
    "                                          n_inputs=len(y_train),\n",
    "                                          outputs=HORIZON,\n",
    "                                          metrics=['mse'])\n",
    "\n",
    "#fit the model\n",
    "history = model_v.fit(x=X_train, \n",
    "                      y=y_train, \n",
    "                      epochs=N_EPOCHS,\n",
    "                      validation_data=(X_test, y_test),\n",
    "                      verbose=0,\n",
    "                      callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.4 Predict a single vector ahead.\n",
    "\n",
    "Predicting a single vector ahead is actually making a h-step forecast.  This is done in exactly the same way as the other models using `.predict(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to understand why we include [0] on the end comment it out and rerun the code.\n",
    "y_preds = model_v.predict(X_test[0].reshape(1, -1))[0]\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the prediction\n",
    "plt.plot(y_test[0], label='test')\n",
    "plt.plot(y_preds, label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.5 Exercise predicting multiple vectors ahead (> h-steps)\n",
    "\n",
    "It is important to remember that with the vector output you predict in multiples of $h$.  So if you make two predictions you have predictions for 2h.  But in general this works in the same way as the iterative method.  Each time you forecast you replace $h$ values in the X vector with the predictions.\n",
    "\n",
    "**Task**: \n",
    "* Modify `autoregressive_iterative_forecast` (provided below) so that it works with the new model.  After you are done rename the function `vector_iterative_forecast`\n",
    "* Predict 4 vector lengths ahead and plot the result against the test set.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* For simplicity, you could make the parameter `h` the number of vectors ahead to predict.\n",
    "* Each call of `model.predict(X)` returns a vector.  At the end of the iterative forecast you will have a list of vectors.  Call `np.concatenate(list)` to transform this into a single list.\n",
    "* In the notebook the X and vectors are both of size 12 (`WINDOW_SIZE == 12` and `len(y_train[0]) == 12`). This means you could simplify your code for the example.  Alternatively it could work with different sized X and y vectors. \n",
    "* Remember that `y_test` contains sliding windows of size 12.  So if you predict 2 vectors ahead then you will need to plot `y_test[0]` and `y_test[12]`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Assumes that h is less than or equal to the length of \n",
    "    exog\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        y_pred = model.predict(current_X.reshape(1, -1))[0]\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "        #update the input array for the iterative prediction\n",
    "        v_len = len(y_pred)\n",
    "        current_X = np.roll(current_X, shift=-v_len)\n",
    "        current_X[-v_len:] = y_pred.copy()\n",
    "        \n",
    "    return np.concatenate(np.array(y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=4\n",
    "y_preds = vector_iterative_forecast(model_v, X_test[0], H)\n",
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_to_plot = []\n",
    "for i in range(H):\n",
    "    y_test_to_plot.append(y_test[WINDOW_SIZE*i])\n",
    "\n",
    "plt.plot(np.concatenate(y_test_to_plot), label='test')\n",
    "plt.plot(y_preds, label='forecast')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
