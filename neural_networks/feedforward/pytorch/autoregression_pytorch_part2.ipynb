{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive models using a feedforward neural network (pytorch implementation)\n",
    "## PART 2: Applying the methods to health care time series \n",
    "\n",
    "In this notebook we will use a feedforward neural network to fit a single and ensemble linear and non-linear models to real time series data. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "1. Most of the work we will do is data manipulation: preprocessing data and making sure it is the right shape for the neural networks.\n",
    "\n",
    "2. The ensemble learning method can be computationally expensive.  We have included some pre-trained models that can be loaded from file if your machine is not powerful or you cannot use Google Collabratory.\n",
    "</div>\n",
    "\n",
    "---\n",
    "**LEARNING OBJECTIVES**\n",
    "\n",
    "* Learn how to apply feedforward neural networks to real health data.\n",
    "* Methods to preprocess nn input data.\n",
    "* Recognise the stochastic nature of neural network training\n",
    "* Use a ensemble of neural networks to provide a more reliable point forecast\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python dependencies\n",
    "\n",
    "It is recommended that you use the forecasting course conda environment provided. We are again going to implement neural networks using `tensorflow` and '`keras`. You should be using at least `tensorflow` version `2.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "#pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forecasting emergency admissions in England\n",
    "\n",
    "We will now use feedforward neural networks to predict the number of monthly emergency admissions in England. \n",
    "\n",
    "### 2.1 Load the data\n",
    "\n",
    "**Task**:\n",
    "* Execute the code below to read the emergency admissions data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/health-data-science-OR/data/master/em_admits_ts.csv'\n",
    "em_admits = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Preprocessing \n",
    "\n",
    "### 2.3.1 Datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the the `month_year` column in `em_admits` holds a string an invalid date format e.g. 'Aug-10'.  Pandas cannot handle this as-is because '10' could refer to any century!  So let's do a bit of preprocessing to get it into a valid datetime format.\n",
    "\n",
    "\n",
    "*Optional Task:*\n",
    "* Take some time to understand the code that preprocesses the dates.  This is real health data and it is likely you will need to deal with formatting issues as experienced here.\n",
    "\n",
    "First we will format the string to something pandas can parse i.e. 'Aug 2010'.  Then we will call the `pd.to_datetime()` function to parse the string and return a `datetime`.  We will assign the result to our dataframe's index and set the freq to monthly start 'MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = em_admits['month_year'].str[:3] + ' 20' + em_admits['month_year'].str[-2:]\n",
    "date_str.name = 'date'\n",
    "em_admits = em_admits.set_index(pd.to_datetime(date_str))\n",
    "em_admits.index.freq = 'MS'\n",
    "em_admits = em_admits.drop(columns=['month_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_admits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Visualise the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be forecasting the last 12 months of the series.  Let's take a look at the training data (being careful to exclude the last 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_length = 12\n",
    "em_admits[:len(em_admits)-holdout_length].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Calender adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is monthly data so a useful preprocessing step is to transform the data into a daily rate by dividing by the number of days in the month. When we plot this the troughs we saw in Feb each year disappear.\n",
    "\n",
    "**Exercise**:\n",
    "* Calculate the average admissions per day series\n",
    "* Plot the training data (holding back 12 months for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate = em_admits['em_admits'] / em_admits.index.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit_rate[:len(admit_rate)-12].plot(figsize=(12,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Convert the time series to format suitable for supervised learning.\n",
    "\n",
    "Exercise:\n",
    "* Using a sliding window approach convert the time series into a tabular format. \n",
    " * Use a window size of 12 and assume you are predicting a scalar value of y (1-step ahead).\n",
    "* Conduct a train test split holding back 12 windows for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(train, window_size=2, horizon=1):\n",
    "    '''\n",
    "    sliding window.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train: array-like\n",
    "        training data for time series method\n",
    "    \n",
    "    window_size: int, optional (default=2)\n",
    "        lookback - how much lagged data to include.\n",
    "        \n",
    "    horizon: int, optional (default=1)\n",
    "        number of observations ahead to predict\n",
    "            \n",
    "    Returns:\n",
    "        array-like, array-like\n",
    "    \n",
    "        preprocessed X, preprocessed Y\n",
    "    '''\n",
    "    tabular_X = []\n",
    "    tabular_y = []\n",
    "    \n",
    "    for i in range(0, len(train) - window_size - horizon):\n",
    "        X_train = train[i:window_size+i]\n",
    "        y_train = train[i+window_size+horizon-1]\n",
    "        tabular_X.append(X_train)\n",
    "        tabular_y.append(y_train)\n",
    "       \n",
    "    return np.asarray(tabular_X), np.asarray(tabular_y).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensors(*arrays):\n",
    "    results = ()\n",
    "    for a in arrays:\n",
    "        results += torch.FloatTensor(a),\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(X, y, batch_size=32):\n",
    "    '''\n",
    "    Set up train data as a TensorDataSet\n",
    "    '''\n",
    "    tensor_data = TensorDataset(torch.FloatTensor(X),\n",
    "                                torch.FloatTensor(y))\n",
    "\n",
    "    return DataLoader(tensor_data, \n",
    "                      batch_size=batch_size, \n",
    "                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(*arrays, train_size, as_tensors=True):\n",
    "    '''\n",
    "    time series train test split\n",
    "    \n",
    "    Parameters:\n",
    "    X: array-like\n",
    "        X data\n",
    "    y_data \n",
    "    '''\n",
    "    results = ()\n",
    "    for a in arrays:\n",
    "        if as_tensors:\n",
    "            results += to_tensors(a[:train_size], a[train_size:])\n",
    "        else:\n",
    "            results += a[:train_size], a[train_size:]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 12\n",
    "\n",
    "#preprocess time series into a supervised learning problem\n",
    "X_data, y_data = sliding_window(admit_rate, window_size=WINDOW_SIZE)\n",
    "\n",
    "#train test split\n",
    "train_size = len(y_data) - 12\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X_data, \n",
    "                                                y_data,\n",
    "                                                train_size=train_size,\n",
    "                                                as_tensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Scaling the features and target to be between -1 and 1\n",
    "In many machine learning applications data are scaled to be between 0 and 1. For neural network forecasting, *Ord, Fildes and Kourentzes (2017)* recommend scaling to be between -1 and 1.  This is what we will do here.  To do the scaling we will use\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler\n",
    "```\n",
    "\n",
    "* Execute the code below to transform the data.\n",
    "\n",
    "**TO ADD: short video or seperate notebook of how min max scaler works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#I am scaling on admit_rate because this will include the first 12 lags \n",
    "#not in y_train\n",
    "scaler.fit(admit_rate.iloc[:-12].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test = scaler.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = to_tensors(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 A Linear regression model benchmark\n",
    "\n",
    "The first model we will try is the linear model. Its will serve as our neural network baseline.  (In practice we would likely use something more complex as a neural network benchmarch and also check this is better than a naive method such as seasonal naive).\n",
    "\n",
    "### 2.4.1. Train the model\n",
    "\n",
    "**Exercise:**\n",
    "* Using `torch`, construct a neural network that mimics a simple linear regression model (see previous notebook).  \n",
    "* Optional: To get comparable results, set the torch random number seed to 1234\n",
    "* Train the model for 100 epochs.\n",
    "* Optionally you can use an early stopping callback with patience set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model on torch.nn.Module class\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, window_size):\n",
    "        # Inherit parent (nn.module) methods using super init\n",
    "        super(LinearModel, self).__init__()\n",
    "        # Linear model only has a single layer \n",
    "        # window_size input containing our lags\n",
    "        #a Linear object is the same as Keras' Dense layer.\n",
    "        self.layer1 = nn.Linear(in_features=window_size, \n",
    "                                out_features=1, \n",
    "                                bias=True)   \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through net. \n",
    "        y_pred = self.layer1(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, criterion, n_epochs, \n",
    "        X_train, y_train, X_test=None, y_test=None, \n",
    "        batch_size=32, verbose=0):\n",
    "    '''\n",
    "    train the pytorch model \n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: torch.nn.module\n",
    "        PyTorch Neural Network Model implements .forward()\n",
    "    \n",
    "    optimizer: torch.optim.Optimizer\n",
    "        PyTorch optimization engine e.g. Adam\n",
    "        \n",
    "    criterion: torch.nn.criterion\n",
    "        PyTorch criterion e.g. MSELoss \n",
    "        \n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "        \n",
    "    X_train: Tensor\n",
    "        x training matrix 2D\n",
    "        \n",
    "    y_train: Tensor\n",
    "        y training vector\n",
    "        \n",
    "    X_test: Tensor, optional (default=None)\n",
    "        x test matrix 2D\n",
    "        \n",
    "    y_test: Tensor, optional (default=None)\n",
    "        y test vector\n",
    "        \n",
    "    batch_size: int, optional (default=32)\n",
    "        Size of the mini batches used in training\n",
    "        \n",
    "    verbose: int, optional (default=0)\n",
    "        0 == no output during training\n",
    "        1 == output loss every 10 epochs \n",
    "        (includes validation loss if X_test, y_test included)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        dict\n",
    "        training and validation loss history. keys are\n",
    "        'loss' and 'val_loss'\n",
    "    '''\n",
    "    PRINT_STEPS = 10\n",
    "\n",
    "    # Set up lists for loss\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    history = {'loss':[],\n",
    "               'val_loss':[]}\n",
    "\n",
    "    #create the mini-batches\n",
    "    train_loader = get_data_loader(X_train, \n",
    "                               y_train, \n",
    "                               batch_size=batch_size)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Loop through required number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Train model (using batches): Switch to training mode\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            y_pred = model.forward(batch[0])\n",
    "            loss = criterion(y_pred, batch[1].reshape(y_pred.shape[0], -1))\n",
    "\n",
    "            # Zero gradients, perform a backward pass,  \n",
    "            # and update the weights. \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "        # Get results for complete training set: Switch to evaluation mode\n",
    "        model.eval()\n",
    "        y_pred_train = model.forward(X_train)\n",
    "        history['loss'].append(criterion(y_pred_train, \n",
    "                                         y_train.reshape(y_pred_train.shape[0], -1)).detach())\n",
    "\n",
    "        if not X_test is None:\n",
    "            # Get results for test set\n",
    "            y_pred_test = model.forward(X_test)\n",
    "            history['val_loss'].append(criterion(y_pred_test, \n",
    "                                                 y_test.reshape(y_pred_test.shape[0], -1)).detach())\n",
    "\n",
    "        # Print loss & accuracy every 10 epochs (print last iteem of results lists)\n",
    "        if verbose == 1:\n",
    "            if (epoch+1) % PRINT_STEPS == 0:\n",
    "                print(f'Epoch {epoch+1}. ', end='')\n",
    "                print(f\"Train accuracy {history['loss'][-1]: 0.3f}. \", end='')\n",
    "                print(f\"Test accuracy {history['val_loss'][-1]: 0.3f}.\")\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    if verbose == 1:\n",
    "        print(f'Training time {duration:.2f}s')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# Create model\n",
    "model_lm = LinearModel(WINDOW_SIZE)\n",
    "# Set loss \n",
    "criterion = nn.MSELoss()\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model_lm.parameters(), lr=0.1)\n",
    "\n",
    "history = fit(model_lm, optimizer, criterion, N_EPOCHS, \n",
    "              X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "              batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], label='loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Plot the fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot(scaler.inverse_transform(y_train), label='ground truth')\n",
    "    plt.plot(scaler.inverse_transform(model_lm(X_train)), label='NN fitted')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Generate and evaluate a multi-step forecast\n",
    "\n",
    "Task:\n",
    "* Using the iterative method produce a 12 step forecast. Save the predictions in a variable called `y_preds_lm`\n",
    "* Plot the results: predictions versus test\n",
    "* Calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, exog, h):\n",
    "    '''\n",
    "    h-step forecast for an autoregressive \n",
    "    model using the iterative prediction method.\n",
    "    \n",
    "    Conduct h one-step forecasts gradually\n",
    "    replacing ground truth autoregressive X \n",
    "    values with predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    ------\n",
    "    model: forecast object\n",
    "        model that has a .predict(h) interface\n",
    "        \n",
    "    exog: array-like\n",
    "        initial vector of lagged values (X)\n",
    "    \n",
    "    h: int\n",
    "        forecast horizon. assumed to be > 0\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    numpy.ndarray\n",
    "        y_predictions\n",
    "    '''\n",
    "    y_preds = []\n",
    "    current_X = exog\n",
    "    for i in range(h):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(current_X)[0]\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        current_X = torch.roll(current_X, shifts=-1)\n",
    "        current_X[-1] = y_pred\n",
    "\n",
    "    return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn_prediction_results(model, X_train, y_train, y_test, y_preds):  \n",
    "    \n",
    "    #create series\n",
    "    with torch.no_grad():\n",
    "        fitted_values = scaler.inverse_transform(model(X_train))\n",
    "        \n",
    "    ground_truth = scaler.inverse_transform(y_train)\n",
    "    ground_truth_val = scaler.inverse_transform(y_test)\n",
    "\n",
    "    padding = np.full(len(fitted_values), np.NAN)\n",
    "\n",
    "    validation = np.concatenate([padding.reshape(-1, 1), ground_truth_val])\n",
    "    forecast = np.concatenate([padding.reshape(-1, 1), y_preds])\n",
    "\n",
    "    plt.plot(ground_truth, label='ground truth')\n",
    "    plt.plot(validation, label='test')\n",
    "    plt.plot(fitted_values, label='in-sample', linestyle='-.')\n",
    "    plt.plot(forecast, label='out-of-sample', linestyle='-.')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict next 12 months and plot\n",
    "H = 12\n",
    "y_preds_lm = autoregressive_iterative_forecast(model_lm, X_test[0], h=H)\n",
    "\n",
    "y_preds_lm = scaler.inverse_transform(y_preds_lm.reshape(-1, 1))\n",
    "\n",
    "plot_nn_prediction_results(model_lm, X_train, y_train, y_test, y_preds_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(y_preds_lm, scaler.inverse_transform(y_test))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Training a non-linear network\n",
    "\n",
    "Task \n",
    "* Introduce a hidden layer(s) to your neural network.  Use a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model on torch.nn.Module class\n",
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, window_size, n_neurons_l1=32,\n",
    "                 n_neurons_l2=64):\n",
    "        # Inherit parent (nn.module) methods using super init\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        #full connected layer\n",
    "        self.fc1 = nn.Linear(in_features=window_size, \n",
    "                             out_features=n_neurons_l1, \n",
    "                             bias=True) \n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=n_neurons_l1, \n",
    "                             out_features=n_neurons_l2, \n",
    "                             bias=True)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=n_neurons_l2, \n",
    "                             out_features=1, \n",
    "                             bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass data through net. \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y_pred = self.out(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(45678)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# Create model\n",
    "model_mlp = TwoLayerModel(WINDOW_SIZE, n_neurons_l1=5, \n",
    "                          n_neurons_l2=5)\n",
    "# Set loss \n",
    "criterion = nn.MSELoss()\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=0.1)\n",
    "\n",
    "history = fit(model_mlp, optimizer, criterion, N_EPOCHS, \n",
    "              X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "              batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'], label='loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict next 12 months and plot\n",
    "H = 12\n",
    "y_preds_mlp = autoregressive_iterative_forecast(model_mlp, X_test[0], h=H)\n",
    "y_preds_mlp = scaler.inverse_transform(y_preds_mlp.reshape(-1, 1))\n",
    "\n",
    "plot_nn_prediction_results(model_mlp, X_train, y_train, y_test, y_preds_mlp)\n",
    "\n",
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mlp = rmse(scaler.inverse_transform(y_test), y_preds_mlp)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse mlp: {rmse_mlp:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try changing the network parameters to see the impact on the \n",
    "#rmse relative to the linear model\n",
    "\n",
    "torch.manual_seed(45678)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "# Create model\n",
    "model_mlp = TwoLayerModel(WINDOW_SIZE, n_neurons_l1=5, \n",
    "                          n_neurons_l2=32)\n",
    "# Set loss \n",
    "criterion = nn.MSELoss()\n",
    "# Set optimizer\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=0.1)\n",
    "\n",
    "history = fit(model_mlp, optimizer, criterion, N_EPOCHS, \n",
    "              X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "              batch_size=32, verbose=0)\n",
    "\n",
    "#predict next 12 months and plot\n",
    "H = 12\n",
    "y_preds_mlp = autoregressive_iterative_forecast(model_mlp, X_test[0], h=H)\n",
    "y_preds_mlp = scaler.inverse_transform(y_preds_mlp.reshape(-1, 1))\n",
    "\n",
    "plot_nn_prediction_results(model_mlp, X_train, y_train, y_test, y_preds_mlp)\n",
    "\n",
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mlp = rmse(scaler.inverse_transform(y_test), y_preds_mlp)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse mlp: {rmse_mlp:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "In all of the examples above we have been setting a random seed for tensorflow.  This 'suggests' that if we used a different randon number seed we would get a slightly different result. Neural networks are extremely flexible and have many parameters. This leads to one of the key challenges with neural networks - overfitting.  There are multiple ways to deal with overfitting.  In forecasting a common approach is to use an **ensemble** of models.  \n",
    "\n",
    "In an ensemble we train multiple models. \n",
    "\n",
    "## Training an ensemble\n",
    "\n",
    "We will train an ensemble of neural networks that mimic a linear model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_ensemble_linear(ws, n_models):\n",
    "    models = []\n",
    "    for n in range(n_models):\n",
    "        model_n = LinearModel(ws)\n",
    "        path = f'./output/ensemble_model{n}.pt'\n",
    "        model_n.load_state_dict(torch.load(path))\n",
    "        model_n.eval()\n",
    "        models.append(model_n)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed so that ensemble can be repeated.\n",
    "torch.manual_seed(1985)\n",
    "\n",
    "N_MODELS = 20\n",
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "#I've pretrained 50 models you can load them from file if wanted.\n",
    "LOAD_FROM_FILE = True\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    #it will take a few seconds to load.\n",
    "    models = load_pretrained_ensemble_linear(WINDOW_SIZE,\n",
    "                                             N_MODELS)\n",
    "    print('Loaded pre-trained models.')\n",
    "else:\n",
    "    models = []\n",
    "    print('Training ensemble =>', end=' ')\n",
    "    for n in range(N_MODELS):\n",
    "        print(f'{n+1}, ', end = ' ')\n",
    "        #single layer nn\n",
    "        model_n = LinearModel(WINDOW_SIZE)\n",
    "        \n",
    "        # Set loss \n",
    "        criterion = nn.MSELoss()\n",
    "        # Set optimizer\n",
    "        optimizer = torch.optim.Adam(model_n.parameters(), lr=0.01)\n",
    "\n",
    "        #fit model silently\n",
    "        history = fit(model_n, optimizer, criterion, N_EPOCHS, \n",
    "                      X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "                      batch_size=32, verbose=0)\n",
    "\n",
    "        path = f'./output/ensemble_model{n}.pt'\n",
    "        torch.save(model_n.state_dict(), path)\n",
    "        \n",
    "        models.append(model_n)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions in an ensemble\n",
    "\n",
    "In an ensemble we predict in a loop. In python this is straightfoward as we simply loop through the models we have trained and call `autoregressive_iterative_forecast`. Store the predictions of each forecast in a python `list`  called `e_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code will take a few seconds to execute\n",
    "H = 12\n",
    "e_preds = []\n",
    "for model in models:\n",
    "    y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "    e_preds.append(y_preds)\n",
    "    \n",
    "e_preds = np.array(e_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform the data and calculate the median and 0.025 and 0.975 percentiles of the point forecasts\n",
    "\n",
    "Remember we can use `scaler.inverse_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_preds = np.asarray(e_preds)\n",
    "e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "y_preds_mdn = np.percentile(e_preds_tran.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_tran.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_tran.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the individual forecasts and the median\n",
    "\n",
    "fig,ax = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "ax[0].plot(e_preds_tran)\n",
    "ax[0].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[0].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(f'Point forecasts: {N_MODELS} models')\n",
    "\n",
    "ax[1].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[1].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[1].plot(y_preds_2_5, label='0.025 percentile', linestyle='-.', color='black')\n",
    "ax[1].plot(y_preds_97_5, label='0.975 percentile', linestyle='--', color='black')\n",
    "#ax[1].plot(y_preds_lm, label='original lmforecast', linestyle='--', color='green')\n",
    "ax[1].set_title(f'Middle 95% of point forecasts ')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mdn = rmse(scaler.inverse_transform(y_test), y_preds_mdn)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse ensemble: {rmse_mdn:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_25 = rmse(scaler.inverse_transform(y_test), y_preds_2_5)[0]\n",
    "rmse_75 = rmse(scaler.inverse_transform(y_test), y_preds_97_5)[0]\n",
    "print(f'95% of linear models will have rmse between: {rmse_75:.2f} - {rmse_25:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Create an ensemble of non-linear models.\n",
    "\n",
    "The two layer model appears to be more accurate than the simple linear regression model and its ensemble counterpart.\n",
    "\n",
    "Task: \n",
    "\n",
    "* Create an ensemble of 20 models.\n",
    "* Use the `get_network_model()` and its parameters to compile your model.\n",
    "* Optional: save your models to file. (recommended)\n",
    "* Forecast the next 12 periods.\n",
    "* Calculate the RMSE of the forecast.\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "* You have **all of the code** you need to complete this task!\n",
    "* Remember to back transform your forecasts\n",
    "* Use the median of the ensemble.\n",
    "* Look carefully at the previous ensemble example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_ensemble_twolayer(ws, l1, l2, \n",
    "                                    n_models):\n",
    "    models = []\n",
    "    for n in range(n_models):\n",
    "        model_n = TwoLayerModel(ws, l1, l2)\n",
    "        path = f'./output/mlp_ensemble_model{n}.pt'\n",
    "        model_n.load_state_dict(torch.load(path))\n",
    "        model_n.eval()\n",
    "        models.append(model_n)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed so that ensemble can be repeated.\n",
    "torch.manual_seed(1985)\n",
    "\n",
    "N_MODELS = 50\n",
    "N_EPOCHS = 100\n",
    "WINDOW_SIZE = 12\n",
    "\n",
    "L1 = 5\n",
    "L2 = 32\n",
    "\n",
    "#I've pretrained 50 models you can load them from file if wanted.\n",
    "LOAD_FROM_FILE = True\n",
    "\n",
    "if LOAD_FROM_FILE:\n",
    "    #it will take a few seconds to load.\n",
    "    models = load_pretrained_ensemble_twolayer(WINDOW_SIZE,\n",
    "                                               L1, L2,\n",
    "                                               N_MODELS)\n",
    "    print('Loaded pretrained models.')\n",
    "else:\n",
    "    models = []\n",
    "    print('Training ensemble =>', end=' ')\n",
    "    for n in range(N_MODELS):\n",
    "        print(f'{n+1}, ', end = ' ')\n",
    "        #two layer nn\n",
    "        model_n = TwoLayerModel(WINDOW_SIZE, n_neurons_l1=L1,\n",
    "                                n_neurons_l2=L2)\n",
    "        \n",
    "        # Set loss \n",
    "        criterion = nn.MSELoss()\n",
    "        # Set optimizer\n",
    "        optimizer = torch.optim.Adam(model_n.parameters(), lr=0.1)\n",
    "\n",
    "        #fit model silently\n",
    "        history = fit(model_n, optimizer, criterion, N_EPOCHS, \n",
    "                      X_train, y_train, X_test=X_test, y_test=y_test, \n",
    "                      batch_size=32, verbose=0)\n",
    "\n",
    "        path = f'./output/mlp_ensemble_model{n}.pt'\n",
    "        torch.save(model_n.state_dict(), path)\n",
    "        \n",
    "        models.append(model_n)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#this code will take a few seconds to execute\n",
    "start_time = time.time()\n",
    "H = 12\n",
    "e_preds = []\n",
    "for model in models[:20]:\n",
    "    y_preds = autoregressive_iterative_forecast(model, X_test[0], h=H)\n",
    "    e_preds.append(y_preds)\n",
    "    \n",
    "e_preds = np.array(e_preds)\n",
    "time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_preds = np.asarray(e_preds)\n",
    "e_preds_tran = scaler.inverse_transform(e_preds).T\n",
    "y_preds_mdn = np.percentile(e_preds_tran.T, 50, axis=0)\n",
    "y_preds_2_5 = np.percentile(e_preds_tran.T, 2.5, axis=0)\n",
    "y_preds_97_5 = np.percentile(e_preds_tran.T, 97.5, axis=0)\n",
    "y_preds_mdn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the individual forecasts and the median\n",
    "\n",
    "fig,ax = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "ax[0].plot(e_preds_tran)\n",
    "ax[0].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[0].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(f'Point forecasts: {N_MODELS} models')\n",
    "\n",
    "ax[1].plot(scaler.inverse_transform(y_test), label='test', linestyle='--', \n",
    "         color='red')\n",
    "ax[1].plot(y_preds_mdn, label='median', linestyle='-', color='black')\n",
    "ax[1].plot(y_preds_2_5, label='0.025 percentile', linestyle='-.', color='black')\n",
    "ax[1].plot(y_preds_97_5, label='0.975 percentile', linestyle='--', color='black')\n",
    "#ax[1].plot(y_preds_lm, label='original lmforecast', linestyle='--', color='green')\n",
    "ax[1].set_title(f'Middle 95% of point forecasts ')\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lm = rmse(scaler.inverse_transform(y_test), y_preds_lm)[0]\n",
    "rmse_mdn = rmse(scaler.inverse_transform(y_test.T), y_preds_mdn)[0]\n",
    "\n",
    "print(f'rmse lm: {rmse_lm:.2f}\\nrmse ensemble: {rmse_mdn:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercises.\n",
    "* How would you use a ensemble method with a model that predicts a vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
